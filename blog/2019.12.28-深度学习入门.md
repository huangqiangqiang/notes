很长一段时间，我都搞不清楚机器学习和深度学习，神经网络之间是什么关系？不懂机器学习可以直接看深度学习吗？神经网络又是啥？后来看了一个大佬的解释才明白，机器学习和深度学习的区别就好像汇编语言和C语言的区别，不懂汇编照样可以学习C语言，可以用C语言做东西，深度学习也一样，当然懂机器学习的话更好。神经网络加深层后就叫做深度学习。

本篇主要是总结最近看的一本《深度学习入门：基于Python的理论与实现》这本书做的一些总结。文章中的图片都是从书中截出来的。这本书非常不错，日本人写的书，讲的很细，从零开始手把手教你撸一个深度学习的神经网络。（而且还有 epub 格式的电子书）

目录：

- [python 基础](#python-基础)
- [感知机](#感知机)
- [神经网络](#神经网络)
  - [三层神经网络](#三层神经网络的实现)
  - [输出层的设计](#输出层的设计)
- [反向传播法](#反向传播法)

# python 基础

书的最开篇，按照惯例，事先学习 python（3.x） 的基本语法和 NumPy 计算库，NumPy 库提供了遍历的矩阵操作方法。还有 Matplotlib 库是用来画图的，可以把一些函数用图片的方式直观的展示出来。

NumPy 和 Matplotlib 都是第三方库，需要用 pip install 一下。

NumPy 库中常用的方法说明：

```
import numpy as np

# 创建一个矩阵
A = np.array([[1,2],[3,4]])
# 矩阵 A 的形状，这里返回 (2, 2) ，表示2行2列的矩阵
A.shape
> (2, 2)
B = np.array([[3,0],[0,6]])

# 相当于是进行普通的两个矩阵乘法运算（A * B）
np.dot(A, B)
> array([[ 3, 12],
       [ 9, 24]])

# 以 0.1 为单位，生成0～5的数据
C = np.arange(0, 6, 0.1)
```

下图是矩阵乘积的计算方法：

<div align="center"><img width="600px" src="./images/2019.12.28-10.jpg"/></div>


Matplotlib 库中常用的方法说明：
```
import matplotlib.pyplot as plt

# 以 0.1 为单位，生成0～5的数据
x = np.arange(0, 6, 0.1)
y = np.sin(x)

# 绘制图形
plt.plot(x, y)
# 显示 x 轴标签
plt.xlabel("x")
# 显示 y 轴标签
plt.ylabel("y")
# 显示图形
plt.show()
```

# 感知机

看完 python 基础，这节我们要来实现一个“感知机”（虽然听不懂，但是好像很牛b的东西）。

感知机是作为神经网络（深度学习）的起源算法。学习感知机能了解神经网络到底是个什么东西。

感知机接收多个输入信号，输出一个信号。这里说的信号可以想像成电流或者河流一样的东西，感知机的信号会形成流，向前方输送信息。但是感知机的信号只有流或不流两种取值（1 或 0），1对应传递信号，0对应不传递信号。

下图是一个有两个输入的感知机的例子：

<div align="center"><img width="600px" src="./images/2019.12.28-1.jpg"/></div>

x1 和 x2 是输入信号，w1 和 w2 是权重，y 是输出。图中的三个圆圈称为神经元或者节点。输入信号传递到神经元时，会分别乘以固定的权重。y神经元会计算传递过来的信号的总和，只有当这个总和超过了某个阈值(theta)，y神经元才会输出1。

用公式表示就是

<div align="center"><img width="600px" src="./images/2019.12.28-2.jpg"/></div>

但是考虑到更通用的形式，我们把式子改成如下形式

<div align="center"><img width="600px" src="./images/2019.12.28-3.jpg"/></div>

θ 符号移到了左边变成了 -θ ，命名为 b ，但表达的内容是完全相同的。w1 和 w2 叫做权重，b 在这里叫做偏置。感知机会计算输入信号和权重的乘积，再加上某个偏置，如果这个值大于0，则输出1，否则输出0。

具体地说，w1 和 w2 是控制输入信号重要性的参数，b 则是调整神经元被激活的容易程度。

那到底感知机有什么用呢？接下来我们用感知机来解决一个简单的问题。

我们用感知机来实现逻辑电路里的"与门"。

与门

| x1 | x2 | y |
|----|----|---|
| 0  | 0 | 0 |
| 0  | 1 | 0 |
| 1  | 0 | 0 |
| 1  | 1 | 1 |

上图中得知，与门是只有在 x1，x2 都为1的情况下，输出才为1。事实上，满足这个条件的参数有无数种，比如 w1，w2 为 0.5，b 为 -0.7，或 w1，w2 为 1.0，b 为 -1.0 都可以。

我们再来看一下"与非门"和"或门",

与非门：

| x1 | x2 | y |
|----|----|---|
| 0  | 0 | 1 |
| 0  | 1 | 1 |
| 1  | 0 | 1 |
| 1  | 1 | 0 |

或门：

| x1 | x2 | y |
|----|----|---|
| 0  | 0 | 0 |
| 0  | 1 | 1 |
| 1  | 0 | 1 |
| 1  | 1 | 1 |

与非门是与门的取反，或门是只要有一个输入信号为1，输出就为1。

那么我们来考虑一下权重w和偏置b。

在 "与非门" 中 w1，w2 为 -0.5，b 为 0.7 就可以，事实上就是与门参数的取反。

"或门" 中 w1，w2 为 1.0，b 为 -0.5 就可以。

这很容易，看图就能看出来。（渐渐骄傲...）

那么，接下来我们来取一下"异或门"的权重和偏置

异或门

| x1 | x2 | y |
|----|----|---|
| 0  | 0 | 0 |
| 0  | 1 | 1 |
| 1  | 0 | 1 |
| 1  | 1 | 0 |

哈哈，想不出来了吧！！！（立即打脸）

事实上，感知机是无法实现这个异或门的！我们可以通过画图来思考其中的原因。

我们看一下 "或门" 的图，如果取 w1，w2 为 1.0，b 为 -0.5，则表达式为

<div align="center"><img width="600px" src="./images/2019.12.28-4.jpg"/></div>

画成图就是这样：

<div align="center"><img width="600px" src="./images/2019.12.28-5.jpg"/></div>

那么，灰色区域就是小于等于0的区域，白色区域就是大于0的区域（输出1的区域）。

图片可以看出，把图片三角形和圆形分割开的直线有无数条，所以权重和偏置也有无数种可能。那么，异或门的图是怎样的呢？

<div align="center"><img width="600px" src="./images/2019.12.28-6.jpg"/></div>

图片可以看到，因为感知机的表达式是一个直线，一条直线是无法把上图中的三角形和圆形分开的，只能用曲线分开，如下图：

<div align="center"><img width="600px" src="./images/2019.12.28-7.jpg"/></div>

感知机的局限就在于只能表示线性函数，这样弯曲的曲线无法表示。另外，由直线能分割的空间称为线性，曲线才能分割的空间称为非线性。线性，非线性这两个术语在机器学习中很常见。

虽然，我们用单个感知机不能表示异或门，但是我们可以通过叠加层，来表示异或门。

如下图所示：

<div align="center"><img width="600px" src="./images/2019.12.28-8.jpg"/></div>

组合 与门，与非门，或门

| x1 | x2 | s1 | s2 | y |
|----|----|---|---|---|
| 0  | 0 | 1 | 0 | 0 |
| 0  | 1 | 1 | 1 | 1 |
| 1  | 0 | 1 | 1 | 1 |
| 1  | 1 | 0 | 1 | 0 |

图中，x1 和 x2 通过 "与非门" ，输出 s1。通过 "或门" 输出 s2。再以 s1，s2 为输入，通过 "与门" ，输出 y。实现了异或门。

以神经元的表示方法显示异或门：

<div align="center"><img width="600px" src="./images/2019.12.28-9.jpg"/></div>

如图，异或门是一个多层结构的神经网络，最左边的输入层称为第0层，中间一列称为第1层，最右边的称为第2层。但是我们在这里把它叫做“2层感知机”，因为有权重的只有两层。

像 "异或门" 这样，还可以解释为“单层感知机无法表示的东西，通过增加一层就可以解决”。也就是说，通过叠加层，可以表示更灵活的东西。

# 神经网络

终于到了神经网络了（看这本书不就是为了这个吗），经过上一章感知机的学习，神经网络和感知机有很多共通点。关于感知机，好消息是感知机可以通过叠加层数，来表达复杂的东西。坏消息是我们需要人工设定权重和偏置。

神经网络的出现就是为了解决刚才的坏消息的。具体地说，神经网络可以自动地从数据中学习到合适和权重和偏置。

来看一个神经网络的例子：

<div align="center"><img width="600px" src="./images/2019.12.28-11.jpg"/></div>

最左边的一列称为输入层，最右边称为输出层，中间的一列称为中间层，也称为隐藏层，因为隐藏层和输入输出层不同，肉眼不可见。也可以把输入层到输出层依次称为第0层，第1层，第2层。上图中一共有3层神经元构成，但实际拥有权重的只有两层，也称为“2层网络”。

然后我们再来看一下之前感知机的公式：

<div align="center"><img width="600px" src="./images/2019.12.28-3.jpg"/></div>

这个式子分为大于0和小于0两种情况，我们用一个函数来表示这种分情况的动作。也可以写成下面这样：

<div align="center"><img width="400px" src="./images/2019.12.28-12.jpg"/></div>

外面套了一个 h() 函数，h() 函数的输入是每个变量的权重和，再经过 h() 内部的转换规则输出 y 值。下图中把偏置b也看作一个输入变量。因为上面的式子可以写成 (b * 1) + (w1 * x1) + (w2 * x2)，输入始终是1，权重为 b 的一个变量。

<div align="center"><img width="400px" src="./images/2019.12.28-13.jpg"/></div>

这个 h() 在神经网络中就是 `激活函数` 。如 激活 一词所示，激活函数的作用在于如何来激活输入信号的总和。

在感知机中，激活函数很简单，就是输入信号的总和大于0，则输出1，小于0则输出0。一旦输入超过阈值，就切换输出，这种也叫阶跃函数。因为函数的图形是阶梯状的，如下图：

<div align="center"><img width="400px" src="./images/2019.12.28-14.jpg"/></div>

事实上，如果将感知机的阶跃函数替换成其他函数，就可以进入神经网络的世界了。

神经网络中经常使用的一个激活函数是 sigmoid 函数，式子是这样的：

<div align="center"><img width="400px" src="./images/2019.12.28-15.jpg"/></div>

式子中，exp(-x) 表示 e的-x次方，e就是那个自然常数，2.7182...那个（头疼）。虽然式子看起来复杂，但也只是个函数而已。而函数就是给定某个值后，经过一定的转换，输出另一个值的转换器。比如，向 sigmoid 输入 1.0 或 2.0 后，就有某个值被输出。类似h(1.0) = 0.731，h(2.0) = 0.880 这样。

<div align="center"><img width="400px" src="./images/2019.12.28-16.jpg"/></div>

sigmoid 函数图形是这样的，y 的取值为 0 ～ 1 之间。

为什么说把阶跃函数换成sigmoid函数就进入了神经网络的世界了呢？我们来比较一下两个函数，看下图：

<div align="center"><img width="400px" src="./images/2019.12.28-17.jpg"/></div>

我们注意到，sigmoid 是一条平滑的曲线，而阶跃函数以0为边界，输出发生急剧的变化。而 sigmoid 函数可以输出连续的值，比如：0.7123，0.812等等。而神经网络中的信号流动也是连续的实数值信号，这对神经网络的学习有重要意义。

注：因为是连续的值，对后面求损失函数的最小值非常有帮助。

sigmoid 函数很早就被开始使用了，但是最近则主要使用 ReLU（Rectified Linear Unit） 函数，图形如下图：

<div align="center"><img width="400px" src="./images/2019.12.28-18.jpg"/></div>

式子是这样：

<div align="center"><img width="400px" src="./images/2019.12.28-19.jpg"/></div>

ReLU 函数在输入大于0时，直接输出该值，输入小于0时，输出0。


### 三层神经网络的实现

上面讲了这么多，接下来，我们用 python3 来实现一个三层的神经网络（激动）

假设我们要实现如下这样结构的一个神经网络：

<div align="center"><img width="400px" src="./images/2019.12.28-20.jpg"/></div>

这个神经网络输入层有两个输入参数，中间层（隐藏层）有两层，第一层隐藏层有3个神经元，第二层隐藏层有2个神经元，输出层有两个神经元。

输入层到隐藏层第一层的信号传递是这样的：

<div align="center"><img width="400px" src="./images/2019.12.28-21.jpg"/></div>

这样可以算出a1

<div align="center"><img width="300px" src="./images/2019.12.28-22.jpg"/></div>

同理，再算出 a2 和 a3 ，需要三步，但是如果使用矩阵的乘法运算，就可以把这三步合为一步。式子如下图所示：

<div align="center"><img width="500px" src="./images/2019.12.28-23.jpg"/></div>

注意这张图中是变量向量乘以权重矩阵W，变量在权重矩阵的左边。式子看不懂也没关系，反正拆开来就是上面那三步。这个矩阵公式用 python 表示出来就是：

```
import numpy as np

A = np.dot(X,W) + B
```

然后算出来的A经过第一层隐藏层神经元内部激活函数 h() 的转换，我们用 z 表示，就是会输出z1，z2，z3，作为第二层隐藏层的输入。这里采用 sigmoid 作为激活函数。用 python 代码表示就是：

```
import numpy as np

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

Z = sigmoid(A)
```

这里的 A 和 Z 都是向量，主要是计算方便，一个式子算出三个值（z1，z2，z3）。

第一层算完了，第二层和第一层的计算方法完全相同。这里就略过了。

<div align="center"><img width="500px" src="./images/2019.12.28-24.jpg"/></div>

最后是第二层到输出层的信号传递，输出层的实现也和之前的大致相同，唯一不同的是激活函数的不同。

为什么激活函数和隐藏层的不同？主要是输出层的激活函数要根据实际求解问题来决定。一般来说，回归问题可以使用恒等函数，二元分类问题使用 sigmoid 函数，多元分类问题使用 softmax 函数。

一下蹦出这么多术语，懵逼了哈哈，接下来就解释下上面那段的意思。

回归问题的意思是根据某个输入预测一个值，比如预测房价，人的体重等等，所以采用恒等函数会输出某个数值。实际上恒等函数就是原样输出，输入啥输出也是啥，啥也不处理。

二元分类问题就是数据属于哪一类的问题，比如区分图片中的人是男人还是女人。因为 sigmoid 会输出 0～1 之间的数，我们就可以拿这个数和 0.5 比大小就行了，大了属于一类，小了就属于另一类。

多元分类问题也是属于哪一类的问题，不过是很多个类别的归类。 softmax 函数其实就是一个计算概率的函数，它的输出是关于输入向量属于每个类别的概率。softmax 函数在后面会详细讲解。

这里，我们先用恒等函数来作为输出层的激活函数，下面是全部的 python 代码，逻辑就是和上面说的一样，很容易懂。

```
import numpy as np

# 激活函数
def sigmoid(x):
  y = 1 / (1 + np.exp(-x))
  return y

# 恒等函数（输出层的激活函数）
def identity_function(x):
  return x

# 初始化权重和偏置信息
def init_network():
  network = {}
  # 第一层的权重
  network["w1"] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
  # 第二层的权重
  network["w2"] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
  # 第三层的权重
  network["w3"] = np.array([[0.1,0.3],[0.2,0.4]])
  # 第一层的偏置
  network["b1"] = np.array([0.1,0.2,0.3])
  # 第二层的偏置
  network["b2"] = np.array([0.1,0.2])
  # 第三层的偏置
  network["b3"] = np.array([0.1,0.2])
  return network

if __name__ == "__main__":
  # 初始化神经网络的权重和偏置信息
  network = init_network()
  # 初始化输入变量
  x = np.array([1.0, 0.5])

  w1, w2, w3 = network["w1"],network["w2"],network["w3"],
  b1, b2, b3 = network["b1"],network["b2"],network["b3"],
  # 计算出第一隐藏层的权重和
  a1 = np.dot(x, w1) + b1
  # 经过第一层激活函数得出输出，就是第二层的输入
  z1 = sigmoid(a1)
  # 和上面一样
  a2 = np.dot(z1, w2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, w3) + b3
  # 最后经过输出层神经元的激活函数（恒等函数），得出最后的结果
  y = identity_function(a3)
```

### 输出层的设计

# 反向传播法

上一章，我们用了数值微分的方法求梯度，然后沿着梯度的方向前进一点点，再求梯度，再前进一点点，如此反复，到达损失函数的最小值的位置。但是这有一个致命的缺点，就是计算速度非常慢，达到了不能忍受的范围。接下来介绍的方向传播法可以用来快速求出梯度。