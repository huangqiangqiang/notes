- 神经网络损失函数
反向传播算法：目的是为下一步梯度下降找出梯度
交叉验证集和训练集有什么区别：因为有多个假设，当训练集拟合好参数theta向量的时候，会得到多个theta向量，然后使用验证集代入每个假设得出的每个假设的误差，然后误差最小的假设就是对这几个假设中验证集拟合最好的假设，因为我们是选择了一个队验证集最拟合的假设，所以相当于我们的假设模型对验证集拟合过一次，所以接下来就不能拿验证集去计算泛化误差，这样会得出过于乐观的估计。

- 最大似然函数：

- 梯度下降法：
标准方程法：可以一次性计算出θ的值，使得损失函数的值最小，不需要多次迭代，也不需要特征缩放。公式是：θ = (X^T * X)^(-1) * X^T * y

- 最小二乘法：
又称为最小平方法（平方在古时候称为二乘），就是把所有误差的平方相加，获得的值为总误差，最小化这个误差就是优化目标。（为什么不是绝对值的和最小）

- 学习型算法概念：
给他一个任务T（下棋）和一个性能测量方法P（和人类棋手对弈的胜率），如果在经验E（程序不断地和自己下棋的经历）的影响下，P对T的测量结果得到了改进，那么就说该程序从E中学习

- 损失函数（代价函数）：（需要取最小值，保证最优）

- 导数：导数不仅仅表示该点切线的斜率，还反应了函数在该点的变化率。（适用在一元函数）
- 偏导数：偏导数仅仅是表示某点在x方向的导数和再y轴方向的导数。（本质和导数一样，适用多元函数）
- 方向导数：我们不仅仅要知道函数在坐标轴方向上的变化率（即偏导数）还需要设法求得函数在其他方向上的变化率。
- 梯度：梯度是众多方向导数中最大的那个向量

- 大O符号：表示时间复杂度，时间复杂度是大概的描述一个算法的用时（实际上从侧面的表达了他的效率）
- 回归问题：我们需要预测的变量是连续的

- 支持向量机：一种算法，可以把数据映射到无限维空间中


- 线性回归
- 逻辑回归：一种分类算法，不要被回归两个字迷惑了。他适用于y为离散值0或1的问题。逻辑回归的预测总是在0和1之间。
- 支持向量机：又称大间距分类器，努力将正样本和负样本用最大的间距（决策边界）分开。

- 多项式：多个单项式的集合，其中单项式的最高次数，为这个多项式的次数

## 一些术语和词汇：
- 神经网络：neural networks
- 激励函数：对类似非线性函数g(z)的另一个称呼，每个神经元应该都有一个激励函数，其实就是逻辑回归函数
- 激励：由一个具体的神经元读入计算并输出值，称为一个激励，比如第二层的第一个激励。
- 前向传播：forward propagation（神经网络中的前向传播意思是从输入层的激励开始，传递给隐藏层并计算隐藏层的激励，再计算输出层的激励。这个流程叫做前向传播。
- 泛化：generalize（指假设模型应用到新样本的能力）
- 假设：hypotheses
- 特征：fealture
- 线性回归：linear regression
- 逻辑回归：logistic regression
- 损失函数：cost function
- 梯度下降（批量梯度下降）：batch gradient descent “批量就是每次都考虑所有的训练样本”
- 随机梯度下降：stochastic gradient descent
- 标准方程：normal equation
- 向量：vectors
- 向量化：vectorizationv
- 矩阵：matrix
- 决策边界： decision boundary
- 正则化：regularized（解决过拟合问题）
- 多项式：
- 过拟合：overfitting
- 欠拟合：underfitting
- 核：kernels
- PCA：Principal Component Analysis 主成分分析方法
- 上限分析：Ceiling Analysis











